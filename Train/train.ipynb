{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e7077d8",
   "metadata": {},
   "source": [
    "# Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9451192",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "\n",
    "# HÃ m helper Ä‘á»ƒ download tá»« Google Drive\n",
    "def download_from_drive(drive_url, local_path):\n",
    "    response = requests.get(drive_url)\n",
    "    response.raise_for_status()\n",
    "    with open(local_path, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "urls = {\n",
    "    \"train\": {\n",
    "        \"sentences\": \"https://drive.google.com/uc?id=1nzak5OkrheRV1ltOGCXkT671bmjODLhP&export=download\",\n",
    "        \"sentiments\": \"https://drive.google.com/uc?id=1ye-gOZIBqXdKOoi_YxvpT6FeRNmViPPv&export=download\",\n",
    "        \"topics\":     \"https://drive.google.com/uc?id=14MuDtwMnNOcr4z_8KdpxprjbwaQ7lJ_C&export=download\",\n",
    "    },\n",
    "    \"validation\": {\n",
    "        \"sentences\": \"https://drive.google.com/uc?id=1sMJSR3oRfPc3fe1gK-V3W5F24tov_517&export=download\",\n",
    "        \"sentiments\": \"https://drive.google.com/uc?id=1GiY1AOp41dLXIIkgES4422AuDwmbUseL&export=download\",\n",
    "        \"topics\":     \"https://drive.google.com/uc?id=1DwLgDEaFWQe8mOd7EpF-xqMEbDLfdT-W&export=download\",\n",
    "    },\n",
    "    \"test\": {\n",
    "        \"sentences\": \"https://drive.google.com/uc?id=1aNMOeZZbNwSRkjyCWAGtNCMa3YrshR-n&export=download\",\n",
    "        \"sentiments\": \"https://drive.google.com/uc?id=1vkQS5gI0is4ACU58-AbWusnemw7KZNfO&export=download\",\n",
    "        \"topics\":     \"https://drive.google.com/uc?id=1_ArMpDguVsbUGl-xSMkTF_p5KpZrmpSB&export=download\",\n",
    "    }\n",
    "}\n",
    "\n",
    "def prepare_split(split_name, urls_for_split, output_dir=\"data\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    paths = {}\n",
    "    for kind, url in urls_for_split.items():\n",
    "        local_path = os.path.join(output_dir, f\"{split_name}_{kind}.txt\")\n",
    "        if not os.path.exists(local_path):\n",
    "            print(f\"Downloading {split_name} {kind} â€¦\")\n",
    "            download_from_drive(url, local_path)\n",
    "        else:\n",
    "            print(f\"File {local_path} Ä‘Ã£ tá»“n táº¡i, bá» qua download.\")\n",
    "        paths[kind] = local_path\n",
    "\n",
    "    # Äá»c file má»—i dÃ²ng lÃ  má»™t báº£n ghi\n",
    "    # DÃ¹ng read_csv vá»›i delimiter máº·c Ä‘á»‹nh (phÃ¢n cÃ¡ch theo dáº¥u pháº©y náº¿u cÃ³),\n",
    "    # á»Ÿ Ä‘Ã¢y file chá»‰ cÃ³ má»™t cá»™t, nÃªn ta chá»‰ cáº§n Ä‘á»c cáº£ dÃ²ng lÃ  string\n",
    "    def read_single_column_txt(path, column_name):\n",
    "        # DÃ¹ng read_csv, má»—i dÃ²ng lÃ  má»™t record\n",
    "        return pd.read_csv(path, header=None, names=[column_name], dtype=str, encoding=\"utf-8\", sep=\"\\r\\n\", engine=\"python\")\n",
    "\n",
    "    # Náº¿u cÃ¡ch trÃªn váº«n lá»—i, dÃ¹ng cÃ¡ch fallback: Ä‘á»c thá»§ cÃ´ng vá»›i Python\n",
    "    def read_single_column_manual(path, column_name):\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            lines = [line.strip(\"\\n\") for line in f]\n",
    "        return pd.DataFrame({column_name: lines})\n",
    "\n",
    "    # Thá»­ Ä‘á»c\n",
    "    try:\n",
    "        df_sent = read_single_column_txt(paths[\"sentences\"], \"sentence\")\n",
    "    except Exception as e:\n",
    "        print(\"Äá»c sentences báº±ng read_csv vá»›i sep=\\\"\\\\r\\\\n\\\" bá»‹ lá»—i, dÃ¹ng manual:\", e)\n",
    "        df_sent = read_single_column_manual(paths[\"sentences\"], \"sentence\")\n",
    "\n",
    "    try:\n",
    "        df_senti = read_single_column_txt(paths[\"sentiments\"], \"sentiment\")\n",
    "    except Exception as e:\n",
    "        print(\"Äá»c sentiments bá»‹ lá»—i, dÃ¹ng manual:\", e)\n",
    "        df_senti = read_single_column_manual(paths[\"sentiments\"], \"sentiment\")\n",
    "\n",
    "    try:\n",
    "        df_topic = read_single_column_txt(paths[\"topics\"], \"topic\")\n",
    "    except Exception as e:\n",
    "        print(\"Äá»c topics bá»‹ lá»—i, dÃ¹ng manual:\", e)\n",
    "        df_topic = read_single_column_manual(paths[\"topics\"], \"topic\")\n",
    "\n",
    "    # Kiá»ƒm tra sá»‘ dÃ²ng\n",
    "    assert len(df_sent) == len(df_senti) == len(df_topic), \\\n",
    "        f\"Sá»‘ dÃ²ng khÃ´ng khá»›p á»Ÿ split {split_name}: sentences {len(df_sent)}, sentiments {len(df_senti)}, topics {len(df_topic)}\"\n",
    "\n",
    "    df = pd.concat([df_sent, df_senti, df_topic], axis=1)\n",
    "\n",
    "    output_csv = os.path.join(output_dir, f\"{split_name}.csv\")\n",
    "    print(f\"LÆ°u file CSV: {output_csv}\")\n",
    "    df.to_csv(output_csv, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def download_and_prepare_all(output_dir=\"data\"):\n",
    "    datasets = {}\n",
    "    for split, split_urls in urls.items():\n",
    "        df = prepare_split(split, split_urls, output_dir=output_dir)\n",
    "        datasets[split] = df\n",
    "    return datasets\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    datasets = download_and_prepare_all(output_dir=\"uit_vsf_feedback_data\")\n",
    "    # Káº¿t quáº£: cÃ³ cÃ¡c file train.csv, validation.csv, test.csv trong thÆ° má»¥c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eebb67c",
   "metadata": {},
   "source": [
    "# TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88f7668",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "\n",
    "# ==============================\n",
    "# 1. Load dataset tá»« CSV\n",
    "# ==============================\n",
    "df_train = pd.read_csv(\"dataset_feedback_students/csv/train.csv\")\n",
    "df_validation = pd.read_csv(\"dataset_feedback_students/csv/validation.csv\")\n",
    "df_test = pd.read_csv(\"dataset_feedback_students/csv/test.csv\")\n",
    "\n",
    "print(\"KÃ­ch thÆ°á»›c táº­p train:\", df_train.shape)\n",
    "print(\"KÃ­ch thÆ°á»›c táº­p validation:\", df_validation.shape)\n",
    "print(\"KÃ­ch thÆ°á»›c táº­p test:\", df_test.shape)\n",
    "\n",
    "# ==============================\n",
    "# 2. Chuáº©n hÃ³a label\n",
    "# ==============================\n",
    "# Encode sentiment -> sá»‘\n",
    "sentiment_encoder = LabelEncoder()\n",
    "df_train['sentiment'] = sentiment_encoder.fit_transform(df_train['sentiment'])\n",
    "df_validation['sentiment'] = sentiment_encoder.transform(df_validation['sentiment'])\n",
    "df_test['sentiment'] = sentiment_encoder.transform(df_test['sentiment'])\n",
    "\n",
    "print(\"CÃ¡c nhÃ£n sentiment:\", list(sentiment_encoder.classes_))\n",
    "\n",
    "# ==============================\n",
    "# 3. Chuáº©n bá»‹ dá»¯ liá»‡u\n",
    "# ==============================\n",
    "X_train_text = df_train['sentence'].tolist()\n",
    "y_train = df_train[['sentiment', 'topic']].values\n",
    "\n",
    "X_validation_text = df_validation['sentence'].tolist()\n",
    "y_validation = df_validation[['sentiment', 'topic']].values\n",
    "\n",
    "X_test_text = df_test['sentence'].tolist()\n",
    "y_test = df_test[['sentiment', 'topic']].values\n",
    "\n",
    "# ==============================\n",
    "# 4. SBERT embedding\n",
    "# ==============================\n",
    "print(\"Äang táº£i model SBERT...\")\n",
    "sbert_model = SentenceTransformer(\"sentence-transformers/LaBSE\")\n",
    "\n",
    "print(\"Äang táº¡o embedding cho dá»¯ liá»‡u train...\")\n",
    "X_train_embeddings = sbert_model.encode(X_train_text, convert_to_numpy=True, show_progress_bar=True)\n",
    "\n",
    "print(\"Äang táº¡o embedding cho dá»¯ liá»‡u validation...\")\n",
    "X_validation_embeddings = sbert_model.encode(X_validation_text, convert_to_numpy=True, show_progress_bar=True)\n",
    "\n",
    "print(\"Äang táº¡o embedding cho dá»¯ liá»‡u test...\")\n",
    "X_test_embeddings = sbert_model.encode(X_test_text, convert_to_numpy=True, show_progress_bar=True)\n",
    "\n",
    "print(\"KÃ­ch thÆ°á»›c embedding train:\", X_train_embeddings.shape)\n",
    "print(\"KÃ­ch thÆ°á»›c embedding validation:\", X_validation_embeddings.shape)\n",
    "print(\"KÃ­ch thÆ°á»›c embedding test:\", X_test_embeddings.shape)\n",
    "\n",
    "# ==============================\n",
    "# 5. Huáº¥n luyá»‡n mÃ´ hÃ¬nh BalancedRandomForest\n",
    "# ==============================\n",
    "brf = BalancedRandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=15,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    sampling_strategy=\"auto\",\n",
    "    replacement=True\n",
    ")\n",
    "\n",
    "\n",
    "model = MultiOutputClassifier(brf)\n",
    "model.fit(X_train_embeddings, y_train)\n",
    "\n",
    "# ==============================\n",
    "# 6. ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh\n",
    "# ==============================\n",
    "print(\"ÄÃ¡nh giÃ¡ trÃªn táº­p validation...\")\n",
    "y_validation_pred = model.predict(X_validation_embeddings)\n",
    "\n",
    "print(\"\\nğŸ“Œ Káº¿t quáº£ trÃªn táº­p validation cho nhÃ£n 'sentiment':\")\n",
    "print(classification_report(y_validation[:, 0], y_validation_pred[:, 0], target_names=sentiment_encoder.classes_))\n",
    "\n",
    "print(\"\\nğŸ“Œ Káº¿t quáº£ trÃªn táº­p validation cho nhÃ£n 'topic':\")\n",
    "print(classification_report(y_validation[:, 1], y_validation_pred[:, 1]))\n",
    "\n",
    "validation_accuracy = (y_validation_pred == y_validation).all(axis=1).mean()\n",
    "print(f\"\\nğŸ¯ Äá»™ chÃ­nh xÃ¡c tá»•ng thá»ƒ trÃªn validation: {validation_accuracy:.4f}\")\n",
    "\n",
    "# ÄÃ¡nh giÃ¡ trÃªn táº­p test\n",
    "print(\"\\nÄÃ¡nh giÃ¡ trÃªn táº­p test...\")\n",
    "y_test_pred = model.predict(X_test_embeddings)\n",
    "\n",
    "print(\"\\nğŸ“Œ Káº¿t quáº£ trÃªn táº­p test cho nhÃ£n 'sentiment':\")\n",
    "print(classification_report(y_test[:, 0], y_test_pred[:, 0], target_names=sentiment_encoder.classes_))\n",
    "\n",
    "print(\"\\nğŸ“Œ Káº¿t quáº£ trÃªn táº­p test cho nhÃ£n 'topic':\")\n",
    "print(classification_report(y_test[:, 1], y_test_pred[:, 1]))\n",
    "\n",
    "test_accuracy = (y_test_pred == y_test).all(axis=1).mean()\n",
    "print(f\"\\nğŸ¯ Äá»™ chÃ­nh xÃ¡c tá»•ng thá»ƒ trÃªn test: {test_accuracy:.4f}\")\n",
    "\n",
    "# ==============================\n",
    "# 7. Váº½ ma tráº­n nháº§m láº«n\n",
    "# ==============================\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# Sentiment\n",
    "plt.subplot(1, 2, 1)\n",
    "cm_sentiment = confusion_matrix(y_test[:, 0], y_test_pred[:, 0])\n",
    "sns.heatmap(cm_sentiment, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=sentiment_encoder.classes_,\n",
    "            yticklabels=sentiment_encoder.classes_)\n",
    "plt.title('Ma tráº­n nháº§m láº«n - Sentiment')\n",
    "plt.ylabel('Thá»±c táº¿')\n",
    "plt.xlabel('Dá»± Ä‘oÃ¡n')\n",
    "\n",
    "# Topic\n",
    "plt.subplot(1, 2, 2)\n",
    "cm_topic = confusion_matrix(y_test[:, 1], y_test_pred[:, 1])\n",
    "sns.heatmap(cm_topic, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Ma tráº­n nháº§m láº«n - Topic')\n",
    "plt.ylabel('Thá»±c táº¿')\n",
    "plt.xlabel('Dá»± Ä‘oÃ¡n')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# # ==============================\n",
    "# # 8. LÆ°u mÃ´ hÃ¬nh + encoder\n",
    "# # ==============================\n",
    "# joblib.dump({\"model\": model, \"sentiment_encoder\": sentiment_encoder}, \"multioutput_brf_model.pkl\")\n",
    "# print(\"âœ… ÄÃ£ lÆ°u mÃ´ hÃ¬nh vÃ o multioutput_brf_model.pkl\")\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 8. LÆ°u mÃ´ hÃ¬nh + encoder + tÃªn SBERT\n",
    "# ==============================\n",
    "save_obj = {\n",
    "    \"model\": model,                         # mÃ´ hÃ¬nh Ä‘Ã£ huáº¥n luyá»‡n\n",
    "    \"sentiment_encoder\": sentiment_encoder, # encoder cho sentiment\n",
    "    \"sbert_model_name\": \"sentence-transformers/LaBSE\"  # chá»‰ cáº§n lÆ°u tÃªn model\n",
    "}\n",
    "\n",
    "joblib.dump(save_obj, \"multioutput_brf_V2_model.pkl\")\n",
    "print(\"âœ… ÄÃ£ lÆ°u mÃ´ hÃ¬nh vÃ o multioutput_brf_V2_model.pkl\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
