{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e7077d8",
   "metadata": {},
   "source": [
    "# Download Dataset +  EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f7638b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pandas numpy scikit-learn matplotlib seaborn underthesea requests joblib imbalanced-learn\n",
    "\n",
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "import requests\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression   # <- đã import\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score\n",
    "import joblib\n",
    "\n",
    "# Thử import RandomOverSampler (imblearn). Nếu không có, sẽ fallback.\n",
    "try:\n",
    "    from imblearn.over_sampling import RandomOverSampler\n",
    "    ROS_AVAILABLE = True\n",
    "except Exception as e:\n",
    "    print(\"⚠ imbalanced-learn (RandomOverSampler) không có. Install bằng: pip install imbalanced-learn\")\n",
    "    ROS_AVAILABLE = False\n",
    "\n",
    "# Thiết lập font (nếu cần hiển thị tiếng Việt)\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "\n",
    "# ---------------------------\n",
    "# 1) Download & Prepare Data\n",
    "# ---------------------------\n",
    "def download_from_drive(drive_url, local_path):\n",
    "    try:\n",
    "        response = requests.get(drive_url, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
    "        with open(local_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"✓ Downloaded: {local_path}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error downloading {local_path}: {e}\")\n",
    "        return False\n",
    "\n",
    "def prepare_dataset():\n",
    "    urls = {\n",
    "        \"train\": {\n",
    "            \"sentences\": \"https://drive.google.com/uc?id=1nzak5OkrheRV1ltOGCXkT671bmjODLhP&export=download\",\n",
    "            \"sentiments\": \"https://drive.google.com/uc?id=1ye-gOZIBqXdKOoi_YxvpT6FeRNmViPPv&export=download\",\n",
    "            \"topics\": \"https://drive.google.com/uc?id=14MuDtwMnNOcr4z_8KdpxprjbwaQ7lJ_C&export=download\",\n",
    "        },\n",
    "        \"validation\": {\n",
    "            \"sentences\": \"https://drive.google.com/uc?id=1sMJSR3oRfPc3fe1gK-V3W5F24tov_517&export=download\",\n",
    "            \"sentiments\": \"https://drive.google.com/uc?id=1GiY1AOp41dLXIIkgES4422AuDwmbUseL&export=download\",\n",
    "            \"topics\": \"https://drive.google.com/uc?id=1DwLgDEaFWQe8mOd7EpF-xqMEbDLfdT-W&export=download\",\n",
    "        },\n",
    "        \"test\": {\n",
    "            \"sentences\": \"https://drive.google.com/uc?id=1aNMOeZZbNwSRkjyCWAGtNCMa3YrshR-n&export=download\",\n",
    "            \"sentiments\": \"https://drive.google.com/uc?id=1vkQS5gI0is4ACU58-AbWusnemw7KZNfO&export=download\",\n",
    "            \"topics\": \"https://drive.google.com/uc?id=1_ArMpDguVsbUGl-xSMkTF_p5KpZrmpSB&export=download\",\n",
    "        }\n",
    "    }\n",
    "\n",
    "    output_dir = \"dataset_feedback_students\"\n",
    "    csv_dir = os.path.join(output_dir, \"csv\")\n",
    "    os.makedirs(csv_dir, exist_ok=True)\n",
    "\n",
    "    datasets = {}\n",
    "\n",
    "    for split_name, split_urls in urls.items():\n",
    "        print(f\"\\n=== Processing {split_name} split ===\")\n",
    "        paths = {}\n",
    "        for data_type, url in split_urls.items():\n",
    "            local_path = os.path.join(output_dir, f\"{split_name}_{data_type}.txt\")\n",
    "            if not os.path.exists(local_path):\n",
    "                if download_from_drive(url, local_path):\n",
    "                    paths[data_type] = local_path\n",
    "                else:\n",
    "                    print(f\"Failed to download {data_type} for {split_name}\")\n",
    "            else:\n",
    "                print(f\"✓ File already exists: {local_path}\")\n",
    "                paths[data_type] = local_path\n",
    "\n",
    "        if len(paths) != 3:\n",
    "            print(f\"Missing files for {split_name}, skipping...\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            with open(paths[\"sentences\"], 'r', encoding='utf-8') as f:\n",
    "                sentences = [line.strip() for line in f.readlines()]\n",
    "            with open(paths[\"sentiments\"], 'r', encoding='utf-8') as f:\n",
    "                sentiments = [line.strip() for line in f.readlines()]\n",
    "            with open(paths[\"topics\"], 'r', encoding='utf-8') as f:\n",
    "                topics = [line.strip() for line in f.readlines()]\n",
    "\n",
    "            if len(sentences) == len(sentiments) == len(topics):\n",
    "                # Map numeric -> text nếu cần\n",
    "                sentiment_map = {'0': 'negative', '1': 'neutral', '2': 'positive'}\n",
    "                sentiments = [sentiment_map.get(s.strip(), s.strip()) for s in sentiments]\n",
    "\n",
    "                df = pd.DataFrame({\n",
    "                    'sentence': sentences,\n",
    "                    'sentiment': sentiments,\n",
    "                    'topic': topics\n",
    "                })\n",
    "\n",
    "                csv_path = os.path.join(csv_dir, f\"{split_name}.csv\")\n",
    "                df.to_csv(csv_path, index=False, encoding='utf-8')\n",
    "                datasets[split_name] = df\n",
    "                print(f\"✓ Saved {split_name}.csv with {len(df)} records\")\n",
    "            else:\n",
    "                print(f\"✗ Data length mismatch in {split_name}: sentences={len(sentences)}, sentiments={len(sentiments)}, topics={len(topics)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error processing {split_name}: {e}\")\n",
    "\n",
    "    return datasets\n",
    "\n",
    "# ---------------------------\n",
    "# 2) Text preprocessing\n",
    "# ---------------------------\n",
    "try:\n",
    "    from underthesea import word_tokenize\n",
    "    USE_UNDERTHESEA = True\n",
    "    print(\"✓ underthesea available\")\n",
    "except Exception:\n",
    "    USE_UNDERTHESEA = False\n",
    "    print(\"⚠ underthesea not available. Using simple tokenization.\")\n",
    "\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text).lower()\n",
    "    text = unicodedata.normalize('NFC', text)\n",
    "    text = re.sub(r'http\\S+|www\\S+', ' ', text)\n",
    "    text = re.sub(r'[@#]\\w+', ' ', text)\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text, flags=re.UNICODE)\n",
    "    text = re.sub(r'\\d+', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def tokenize_vietnamese(text):\n",
    "    text = clean_text(text)\n",
    "    if USE_UNDERTHESEA and text:\n",
    "        try:\n",
    "            return word_tokenize(text, format=\"text\")\n",
    "        except:\n",
    "            pass\n",
    "    return \" \".join(text.split())\n",
    "\n",
    "# ---------------------------\n",
    "# 3) EDA helper\n",
    "# ---------------------------\n",
    "def perform_eda(df, name=\"dataset\"):\n",
    "    print(f\"\\n=== EDA for {name} ===\")\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(\"\\nSentiment distribution:\")\n",
    "    print(df['sentiment'].value_counts().sort_index())\n",
    "    df['sentence_length'] = df['sentence'].astype(str).apply(lambda x: len(x.split()))\n",
    "    print(f\"\\nSentence length statistics:\")\n",
    "    print(df['sentence_length'].describe())\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18,5))\n",
    "    sentiment_counts = df['sentiment'].value_counts().sort_index()\n",
    "    axes[0].bar(sentiment_counts.index, sentiment_counts.values)\n",
    "    axes[0].set_title(f'{name} - Sentiment Distribution')\n",
    "    axes[0].set_xlabel('Sentiment')\n",
    "    axes[0].set_ylabel('Count')\n",
    "\n",
    "    top_topics = df['topic'].value_counts().head(10)\n",
    "    axes[1].barh(range(len(top_topics)), top_topics.values)\n",
    "    axes[1].set_yticks(range(len(top_topics)))\n",
    "    axes[1].set_yticklabels(top_topics.index)\n",
    "    axes[1].set_title(f'{name} - Top 10 Topics')\n",
    "    axes[1].set_xlabel('Count')\n",
    "\n",
    "    axes[2].hist(df['sentence_length'], bins=30, alpha=0.7)\n",
    "    axes[2].set_title(f'{name} - Sentence Length Distribution')\n",
    "    axes[2].set_xlabel('Sentence Length (words)')\n",
    "    axes[2].set_ylabel('Frequency')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return df\n",
    "\n",
    "def predict_sentiment(text, model, vectorizer):\n",
    "    text_clean = tokenize_vietnamese(text)\n",
    "    X = vectorizer.transform([text_clean])\n",
    "    pred = model.predict(X)[0]\n",
    "    prob = None\n",
    "    try:\n",
    "        prob = model.predict_proba(X)[0]\n",
    "    except:\n",
    "        pass\n",
    "    label_names = ['negative','neutral','positive']\n",
    "    return {\n",
    "        'prediction': label_names[pred],\n",
    "        'confidence': float(max(prob)) if prob is not None else None,\n",
    "        'probabilities': dict(zip(label_names, prob)) if prob is not None else None\n",
    "    }\n",
    "\n",
    "# ---------------------------\n",
    "# 6) Main\n",
    "# ---------------------------\n",
    "def main():\n",
    "    print(\"=== Vietnamese Student Feedback Sentiment Analysis (Oversampling) ===\")\n",
    "    datasets = prepare_dataset()\n",
    "    if len(datasets) < 3:\n",
    "        print(\"✗ Not enough datasets. Aborting.\")\n",
    "        return\n",
    "\n",
    "    train_df = datasets['train']\n",
    "    val_df = datasets['validation']\n",
    "    test_df = datasets['test']\n",
    "    print(f\"✓ Loaded datasets - Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n",
    "\n",
    "    train_df = perform_eda(train_df, \"Training\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "       main()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n⚠ Interrupted by user\")\n",
    "    except Exception as e:\n",
    "        print(\"\\n✗ Error during execution:\", e)\n",
    "        import traceback; traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eebb67c",
   "metadata": {},
   "source": [
    "# TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88f7668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "\n",
    "# ==============================\n",
    "# 1. Load dataset từ CSV\n",
    "# ==============================\n",
    "df_train = pd.read_csv(\"dataset_feedback_students/csv/train.csv\")\n",
    "df_validation = pd.read_csv(\"dataset_feedback_students/csv/validation.csv\")\n",
    "df_test = pd.read_csv(\"dataset_feedback_students/csv/test.csv\")\n",
    "\n",
    "print(\"Kích thước tập train:\", df_train.shape)\n",
    "print(\"Kích thước tập validation:\", df_validation.shape)\n",
    "print(\"Kích thước tập test:\", df_test.shape)\n",
    "\n",
    "# ==============================\n",
    "# 2. Chuẩn hóa label\n",
    "# ==============================\n",
    "# Encode sentiment -> số\n",
    "sentiment_encoder = LabelEncoder()\n",
    "df_train['sentiment'] = sentiment_encoder.fit_transform(df_train['sentiment'])\n",
    "df_validation['sentiment'] = sentiment_encoder.transform(df_validation['sentiment'])\n",
    "df_test['sentiment'] = sentiment_encoder.transform(df_test['sentiment'])\n",
    "\n",
    "print(\"Các nhãn sentiment:\", list(sentiment_encoder.classes_))\n",
    "\n",
    "# ==============================\n",
    "# 3. Chuẩn bị dữ liệu\n",
    "# ==============================\n",
    "X_train_text = df_train['sentence'].tolist()\n",
    "y_train = df_train[['sentiment', 'topic']].values\n",
    "\n",
    "X_validation_text = df_validation['sentence'].tolist()\n",
    "y_validation = df_validation[['sentiment', 'topic']].values\n",
    "\n",
    "X_test_text = df_test['sentence'].tolist()\n",
    "y_test = df_test[['sentiment', 'topic']].values\n",
    "\n",
    "# ==============================\n",
    "# 4. SBERT embedding\n",
    "# ==============================\n",
    "print(\"Đang tải model SBERT...\")\n",
    "sbert_model = SentenceTransformer(\"sentence-transformers/LaBSE\")\n",
    "\n",
    "print(\"Đang tạo embedding cho dữ liệu train...\")\n",
    "X_train_embeddings = sbert_model.encode(X_train_text, convert_to_numpy=True, show_progress_bar=True)\n",
    "\n",
    "print(\"Đang tạo embedding cho dữ liệu validation...\")\n",
    "X_validation_embeddings = sbert_model.encode(X_validation_text, convert_to_numpy=True, show_progress_bar=True)\n",
    "\n",
    "print(\"Đang tạo embedding cho dữ liệu test...\")\n",
    "X_test_embeddings = sbert_model.encode(X_test_text, convert_to_numpy=True, show_progress_bar=True)\n",
    "\n",
    "print(\"Kích thước embedding train:\", X_train_embeddings.shape)\n",
    "print(\"Kích thước embedding validation:\", X_validation_embeddings.shape)\n",
    "print(\"Kích thước embedding test:\", X_test_embeddings.shape)\n",
    "\n",
    "# ==============================\n",
    "# 5. Huấn luyện mô hình BalancedRandomForest\n",
    "# ==============================\n",
    "brf = BalancedRandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=15,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    sampling_strategy=\"auto\",\n",
    "    replacement=True\n",
    ")\n",
    "\n",
    "\n",
    "model = MultiOutputClassifier(brf)\n",
    "model.fit(X_train_embeddings, y_train)\n",
    "\n",
    "# ==============================\n",
    "# 6. Đánh giá mô hình\n",
    "# ==============================\n",
    "print(\"Đánh giá trên tập validation...\")\n",
    "y_validation_pred = model.predict(X_validation_embeddings)\n",
    "\n",
    "print(\"\\n📌 Kết quả trên tập validation cho nhãn 'sentiment':\")\n",
    "print(classification_report(y_validation[:, 0], y_validation_pred[:, 0], target_names=sentiment_encoder.classes_))\n",
    "\n",
    "print(\"\\n📌 Kết quả trên tập validation cho nhãn 'topic':\")\n",
    "print(classification_report(y_validation[:, 1], y_validation_pred[:, 1]))\n",
    "\n",
    "validation_accuracy = (y_validation_pred == y_validation).all(axis=1).mean()\n",
    "print(f\"\\n🎯 Độ chính xác tổng thể trên validation: {validation_accuracy:.4f}\")\n",
    "\n",
    "# Đánh giá trên tập test\n",
    "print(\"\\nĐánh giá trên tập test...\")\n",
    "y_test_pred = model.predict(X_test_embeddings)\n",
    "\n",
    "print(\"\\n📌 Kết quả trên tập test cho nhãn 'sentiment':\")\n",
    "print(classification_report(y_test[:, 0], y_test_pred[:, 0], target_names=sentiment_encoder.classes_))\n",
    "\n",
    "print(\"\\n📌 Kết quả trên tập test cho nhãn 'topic':\")\n",
    "print(classification_report(y_test[:, 1], y_test_pred[:, 1]))\n",
    "\n",
    "test_accuracy = (y_test_pred == y_test).all(axis=1).mean()\n",
    "print(f\"\\n🎯 Độ chính xác tổng thể trên test: {test_accuracy:.4f}\")\n",
    "\n",
    "# ==============================\n",
    "# 7. Vẽ ma trận nhầm lẫn\n",
    "# ==============================\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# Sentiment\n",
    "plt.subplot(1, 2, 1)\n",
    "cm_sentiment = confusion_matrix(y_test[:, 0], y_test_pred[:, 0])\n",
    "sns.heatmap(cm_sentiment, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=sentiment_encoder.classes_,\n",
    "            yticklabels=sentiment_encoder.classes_)\n",
    "plt.title('Ma trận nhầm lẫn - Sentiment')\n",
    "plt.ylabel('Thực tế')\n",
    "plt.xlabel('Dự đoán')\n",
    "\n",
    "# Topic\n",
    "plt.subplot(1, 2, 2)\n",
    "cm_topic = confusion_matrix(y_test[:, 1], y_test_pred[:, 1])\n",
    "sns.heatmap(cm_topic, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Ma trận nhầm lẫn - Topic')\n",
    "plt.ylabel('Thực tế')\n",
    "plt.xlabel('Dự đoán')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# # ==============================\n",
    "# # 8. Lưu mô hình + encoder\n",
    "# # ==============================\n",
    "# joblib.dump({\"model\": model, \"sentiment_encoder\": sentiment_encoder}, \"multioutput_brf_model.pkl\")\n",
    "# print(\"✅ Đã lưu mô hình vào multioutput_brf_model.pkl\")\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 8. Lưu mô hình + encoder + tên SBERT\n",
    "# ==============================\n",
    "save_obj = {\n",
    "    \"model\": model,                         # mô hình đã huấn luyện\n",
    "    \"sentiment_encoder\": sentiment_encoder, # encoder cho sentiment\n",
    "    \"sbert_model_name\": \"sentence-transformers/LaBSE\"  # chỉ cần lưu tên model\n",
    "}\n",
    "\n",
    "joblib.dump(save_obj, \"multioutput_brf_V2_model.pkl\")\n",
    "print(\"✅ Đã lưu mô hình vào multioutput_brf_V2_model.pkl\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envBTL_ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
