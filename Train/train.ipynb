{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e7077d8",
   "metadata": {},
   "source": [
    "# Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9451192",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "\n",
    "# H√†m helper ƒë·ªÉ download t·ª´ Google Drive\n",
    "def download_from_drive(drive_url, local_path):\n",
    "    response = requests.get(drive_url)\n",
    "    response.raise_for_status()\n",
    "    with open(local_path, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "urls = {\n",
    "    \"train\": {\n",
    "        \"sentences\": \"https://drive.google.com/uc?id=1nzak5OkrheRV1ltOGCXkT671bmjODLhP&export=download\",\n",
    "        \"sentiments\": \"https://drive.google.com/uc?id=1ye-gOZIBqXdKOoi_YxvpT6FeRNmViPPv&export=download\",\n",
    "        \"topics\":     \"https://drive.google.com/uc?id=14MuDtwMnNOcr4z_8KdpxprjbwaQ7lJ_C&export=download\",\n",
    "    },\n",
    "    \"validation\": {\n",
    "        \"sentences\": \"https://drive.google.com/uc?id=1sMJSR3oRfPc3fe1gK-V3W5F24tov_517&export=download\",\n",
    "        \"sentiments\": \"https://drive.google.com/uc?id=1GiY1AOp41dLXIIkgES4422AuDwmbUseL&export=download\",\n",
    "        \"topics\":     \"https://drive.google.com/uc?id=1DwLgDEaFWQe8mOd7EpF-xqMEbDLfdT-W&export=download\",\n",
    "    },\n",
    "    \"test\": {\n",
    "        \"sentences\": \"https://drive.google.com/uc?id=1aNMOeZZbNwSRkjyCWAGtNCMa3YrshR-n&export=download\",\n",
    "        \"sentiments\": \"https://drive.google.com/uc?id=1vkQS5gI0is4ACU58-AbWusnemw7KZNfO&export=download\",\n",
    "        \"topics\":     \"https://drive.google.com/uc?id=1_ArMpDguVsbUGl-xSMkTF_p5KpZrmpSB&export=download\",\n",
    "    }\n",
    "}\n",
    "\n",
    "def prepare_split(split_name, urls_for_split, output_dir=\"data\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    paths = {}\n",
    "    for kind, url in urls_for_split.items():\n",
    "        local_path = os.path.join(output_dir, f\"{split_name}_{kind}.txt\")\n",
    "        if not os.path.exists(local_path):\n",
    "            print(f\"Downloading {split_name} {kind} ‚Ä¶\")\n",
    "            download_from_drive(url, local_path)\n",
    "        else:\n",
    "            print(f\"File {local_path} ƒë√£ t·ªìn t·∫°i, b·ªè qua download.\")\n",
    "        paths[kind] = local_path\n",
    "\n",
    "    # ƒê·ªçc file m·ªói d√≤ng l√† m·ªôt b·∫£n ghi\n",
    "    # D√πng read_csv v·ªõi delimiter m·∫∑c ƒë·ªãnh (ph√¢n c√°ch theo d·∫•u ph·∫©y n·∫øu c√≥),\n",
    "    # ·ªü ƒë√¢y file ch·ªâ c√≥ m·ªôt c·ªôt, n√™n ta ch·ªâ c·∫ßn ƒë·ªçc c·∫£ d√≤ng l√† string\n",
    "    def read_single_column_txt(path, column_name):\n",
    "        # D√πng read_csv, m·ªói d√≤ng l√† m·ªôt record\n",
    "        return pd.read_csv(path, header=None, names=[column_name], dtype=str, encoding=\"utf-8\", sep=\"\\r\\n\", engine=\"python\")\n",
    "\n",
    "    # N·∫øu c√°ch tr√™n v·∫´n l·ªói, d√πng c√°ch fallback: ƒë·ªçc th·ªß c√¥ng v·ªõi Python\n",
    "    def read_single_column_manual(path, column_name):\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            lines = [line.strip(\"\\n\") for line in f]\n",
    "        return pd.DataFrame({column_name: lines})\n",
    "\n",
    "    # Th·ª≠ ƒë·ªçc\n",
    "    try:\n",
    "        df_sent = read_single_column_txt(paths[\"sentences\"], \"sentence\")\n",
    "    except Exception as e:\n",
    "        print(\"ƒê·ªçc sentences b·∫±ng read_csv v·ªõi sep=\\\"\\\\r\\\\n\\\" b·ªã l·ªói, d√πng manual:\", e)\n",
    "        df_sent = read_single_column_manual(paths[\"sentences\"], \"sentence\")\n",
    "\n",
    "    try:\n",
    "        df_senti = read_single_column_txt(paths[\"sentiments\"], \"sentiment\")\n",
    "    except Exception as e:\n",
    "        print(\"ƒê·ªçc sentiments b·ªã l·ªói, d√πng manual:\", e)\n",
    "        df_senti = read_single_column_manual(paths[\"sentiments\"], \"sentiment\")\n",
    "\n",
    "    try:\n",
    "        df_topic = read_single_column_txt(paths[\"topics\"], \"topic\")\n",
    "    except Exception as e:\n",
    "        print(\"ƒê·ªçc topics b·ªã l·ªói, d√πng manual:\", e)\n",
    "        df_topic = read_single_column_manual(paths[\"topics\"], \"topic\")\n",
    "\n",
    "    # Ki·ªÉm tra s·ªë d√≤ng\n",
    "    assert len(df_sent) == len(df_senti) == len(df_topic), \\\n",
    "        f\"S·ªë d√≤ng kh√¥ng kh·ªõp ·ªü split {split_name}: sentences {len(df_sent)}, sentiments {len(df_senti)}, topics {len(df_topic)}\"\n",
    "\n",
    "    df = pd.concat([df_sent, df_senti, df_topic], axis=1)\n",
    "\n",
    "    output_csv = os.path.join(output_dir, f\"{split_name}.csv\")\n",
    "    print(f\"L∆∞u file CSV: {output_csv}\")\n",
    "    df.to_csv(output_csv, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def download_and_prepare_all(output_dir=\"data\"):\n",
    "    datasets = {}\n",
    "    for split, split_urls in urls.items():\n",
    "        df = prepare_split(split, split_urls, output_dir=output_dir)\n",
    "        datasets[split] = df\n",
    "    return datasets\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    datasets = download_and_prepare_all(output_dir=\"uit_vsf_feedback_data\")\n",
    "    # K·∫øt qu·∫£: c√≥ c√°c file train.csv, validation.csv, test.csv trong th∆∞ m·ª•c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eebb67c",
   "metadata": {},
   "source": [
    "# TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88f7668",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "\n",
    "# ==============================\n",
    "# 1. Load dataset t·ª´ CSV\n",
    "# ==============================\n",
    "df_train = pd.read_csv(\"dataset_feedback_students/csv/train.csv\")\n",
    "df_validation = pd.read_csv(\"dataset_feedback_students/csv/validation.csv\")\n",
    "df_test = pd.read_csv(\"dataset_feedback_students/csv/test.csv\")\n",
    "\n",
    "print(\"K√≠ch th∆∞·ªõc t·∫≠p train:\", df_train.shape)\n",
    "print(\"K√≠ch th∆∞·ªõc t·∫≠p validation:\", df_validation.shape)\n",
    "print(\"K√≠ch th∆∞·ªõc t·∫≠p test:\", df_test.shape)\n",
    "\n",
    "# ==============================\n",
    "# 2. Chu·∫©n h√≥a label\n",
    "# ==============================\n",
    "# Encode sentiment -> s·ªë\n",
    "sentiment_encoder = LabelEncoder()\n",
    "df_train['sentiment'] = sentiment_encoder.fit_transform(df_train['sentiment'])\n",
    "df_validation['sentiment'] = sentiment_encoder.transform(df_validation['sentiment'])\n",
    "df_test['sentiment'] = sentiment_encoder.transform(df_test['sentiment'])\n",
    "\n",
    "print(\"C√°c nh√£n sentiment:\", list(sentiment_encoder.classes_))\n",
    "\n",
    "# ==============================\n",
    "# 3. Chu·∫©n b·ªã d·ªØ li·ªáu\n",
    "# ==============================\n",
    "X_train_text = df_train['sentence'].tolist()\n",
    "y_train = df_train[['sentiment', 'topic']].values\n",
    "\n",
    "X_validation_text = df_validation['sentence'].tolist()\n",
    "y_validation = df_validation[['sentiment', 'topic']].values\n",
    "\n",
    "X_test_text = df_test['sentence'].tolist()\n",
    "y_test = df_test[['sentiment', 'topic']].values\n",
    "\n",
    "# ==============================\n",
    "# 4. SBERT embedding\n",
    "# ==============================\n",
    "print(\"ƒêang t·∫£i model SBERT...\")\n",
    "sbert_model = SentenceTransformer(\"sentence-transformers/LaBSE\")\n",
    "\n",
    "print(\"ƒêang t·∫°o embedding cho d·ªØ li·ªáu train...\")\n",
    "X_train_embeddings = sbert_model.encode(X_train_text, convert_to_numpy=True, show_progress_bar=True)\n",
    "\n",
    "print(\"ƒêang t·∫°o embedding cho d·ªØ li·ªáu validation...\")\n",
    "X_validation_embeddings = sbert_model.encode(X_validation_text, convert_to_numpy=True, show_progress_bar=True)\n",
    "\n",
    "print(\"ƒêang t·∫°o embedding cho d·ªØ li·ªáu test...\")\n",
    "X_test_embeddings = sbert_model.encode(X_test_text, convert_to_numpy=True, show_progress_bar=True)\n",
    "\n",
    "print(\"K√≠ch th∆∞·ªõc embedding train:\", X_train_embeddings.shape)\n",
    "print(\"K√≠ch th∆∞·ªõc embedding validation:\", X_validation_embeddings.shape)\n",
    "print(\"K√≠ch th∆∞·ªõc embedding test:\", X_test_embeddings.shape)\n",
    "\n",
    "# ==============================\n",
    "# 5. Hu·∫•n luy·ªán m√¥ h√¨nh BalancedRandomForest\n",
    "# ==============================\n",
    "brf = BalancedRandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=15,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    sampling_strategy=\"auto\",\n",
    "    replacement=True\n",
    ")\n",
    "\n",
    "\n",
    "model = MultiOutputClassifier(brf)\n",
    "model.fit(X_train_embeddings, y_train)\n",
    "\n",
    "# ==============================\n",
    "# 6. ƒê√°nh gi√° m√¥ h√¨nh\n",
    "# ==============================\n",
    "print(\"ƒê√°nh gi√° tr√™n t·∫≠p validation...\")\n",
    "y_validation_pred = model.predict(X_validation_embeddings)\n",
    "\n",
    "print(\"\\nüìå K·∫øt qu·∫£ tr√™n t·∫≠p validation cho nh√£n 'sentiment':\")\n",
    "print(classification_report(y_validation[:, 0], y_validation_pred[:, 0], target_names=sentiment_encoder.classes_))\n",
    "\n",
    "print(\"\\nüìå K·∫øt qu·∫£ tr√™n t·∫≠p validation cho nh√£n 'topic':\")\n",
    "print(classification_report(y_validation[:, 1], y_validation_pred[:, 1]))\n",
    "\n",
    "validation_accuracy = (y_validation_pred == y_validation).all(axis=1).mean()\n",
    "print(f\"\\nüéØ ƒê·ªô ch√≠nh x√°c t·ªïng th·ªÉ tr√™n validation: {validation_accuracy:.4f}\")\n",
    "\n",
    "# ƒê√°nh gi√° tr√™n t·∫≠p test\n",
    "print(\"\\nƒê√°nh gi√° tr√™n t·∫≠p test...\")\n",
    "y_test_pred = model.predict(X_test_embeddings)\n",
    "\n",
    "print(\"\\nüìå K·∫øt qu·∫£ tr√™n t·∫≠p test cho nh√£n 'sentiment':\")\n",
    "print(classification_report(y_test[:, 0], y_test_pred[:, 0], target_names=sentiment_encoder.classes_))\n",
    "\n",
    "print(\"\\nüìå K·∫øt qu·∫£ tr√™n t·∫≠p test cho nh√£n 'topic':\")\n",
    "print(classification_report(y_test[:, 1], y_test_pred[:, 1]))\n",
    "\n",
    "test_accuracy = (y_test_pred == y_test).all(axis=1).mean()\n",
    "print(f\"\\nüéØ ƒê·ªô ch√≠nh x√°c t·ªïng th·ªÉ tr√™n test: {test_accuracy:.4f}\")\n",
    "\n",
    "# ==============================\n",
    "# 7. V·∫Ω ma tr·∫≠n nh·∫ßm l·∫´n\n",
    "# ==============================\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# Sentiment\n",
    "plt.subplot(1, 2, 1)\n",
    "cm_sentiment = confusion_matrix(y_test[:, 0], y_test_pred[:, 0])\n",
    "sns.heatmap(cm_sentiment, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=sentiment_encoder.classes_,\n",
    "            yticklabels=sentiment_encoder.classes_)\n",
    "plt.title('Ma tr·∫≠n nh·∫ßm l·∫´n - Sentiment')\n",
    "plt.ylabel('Th·ª±c t·∫ø')\n",
    "plt.xlabel('D·ª± ƒëo√°n')\n",
    "\n",
    "# Topic\n",
    "plt.subplot(1, 2, 2)\n",
    "cm_topic = confusion_matrix(y_test[:, 1], y_test_pred[:, 1])\n",
    "sns.heatmap(cm_topic, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Ma tr·∫≠n nh·∫ßm l·∫´n - Topic')\n",
    "plt.ylabel('Th·ª±c t·∫ø')\n",
    "plt.xlabel('D·ª± ƒëo√°n')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# # ==============================\n",
    "# # 8. L∆∞u m√¥ h√¨nh + encoder\n",
    "# # ==============================\n",
    "# joblib.dump({\"model\": model, \"sentiment_encoder\": sentiment_encoder}, \"multioutput_brf_model.pkl\")\n",
    "# print(\"‚úÖ ƒê√£ l∆∞u m√¥ h√¨nh v√†o multioutput_brf_model.pkl\")\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 8. L∆∞u m√¥ h√¨nh + encoder + t√™n SBERT\n",
    "# ==============================\n",
    "save_obj = {\n",
    "    \"model\": model,                         # m√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán\n",
    "    \"sentiment_encoder\": sentiment_encoder, # encoder cho sentiment\n",
    "    \"sbert_model_name\": \"sentence-transformers/LaBSE\"  # ch·ªâ c·∫ßn l∆∞u t√™n model\n",
    "}\n",
    "\n",
    "joblib.dump(save_obj, \"multioutput_brf_V2_model.pkl\")\n",
    "print(\"‚úÖ ƒê√£ l∆∞u m√¥ h√¨nh v√†o multioutput_brf_V2_model.pkl\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
