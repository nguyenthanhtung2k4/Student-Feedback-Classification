{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e7077d8",
   "metadata": {},
   "source": [
    "# Download Dataset +  EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f7638b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pandas numpy scikit-learn matplotlib seaborn underthesea requests joblib imbalanced-learn\n",
    "\n",
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "import requests\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression   # <- ƒë√£ import\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score\n",
    "import joblib\n",
    "\n",
    "# Th·ª≠ import RandomOverSampler (imblearn). N·∫øu kh√¥ng c√≥, s·∫Ω fallback.\n",
    "try:\n",
    "    from imblearn.over_sampling import RandomOverSampler\n",
    "    ROS_AVAILABLE = True\n",
    "except Exception as e:\n",
    "    print(\"‚ö† imbalanced-learn (RandomOverSampler) kh√¥ng c√≥. Install b·∫±ng: pip install imbalanced-learn\")\n",
    "    ROS_AVAILABLE = False\n",
    "\n",
    "# Thi·∫øt l·∫≠p font (n·∫øu c·∫ßn hi·ªÉn th·ªã ti·∫øng Vi·ªát)\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "\n",
    "# ---------------------------\n",
    "# 1) Download & Prepare Data\n",
    "# ---------------------------\n",
    "def download_from_drive(drive_url, local_path):\n",
    "    try:\n",
    "        response = requests.get(drive_url, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
    "        with open(local_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"‚úì Downloaded: {local_path}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Error downloading {local_path}: {e}\")\n",
    "        return False\n",
    "\n",
    "def prepare_dataset():\n",
    "    urls = {\n",
    "        \"train\": {\n",
    "            \"sentences\": \"https://drive.google.com/uc?id=1nzak5OkrheRV1ltOGCXkT671bmjODLhP&export=download\",\n",
    "            \"sentiments\": \"https://drive.google.com/uc?id=1ye-gOZIBqXdKOoi_YxvpT6FeRNmViPPv&export=download\",\n",
    "            \"topics\": \"https://drive.google.com/uc?id=14MuDtwMnNOcr4z_8KdpxprjbwaQ7lJ_C&export=download\",\n",
    "        },\n",
    "        \"validation\": {\n",
    "            \"sentences\": \"https://drive.google.com/uc?id=1sMJSR3oRfPc3fe1gK-V3W5F24tov_517&export=download\",\n",
    "            \"sentiments\": \"https://drive.google.com/uc?id=1GiY1AOp41dLXIIkgES4422AuDwmbUseL&export=download\",\n",
    "            \"topics\": \"https://drive.google.com/uc?id=1DwLgDEaFWQe8mOd7EpF-xqMEbDLfdT-W&export=download\",\n",
    "        },\n",
    "        \"test\": {\n",
    "            \"sentences\": \"https://drive.google.com/uc?id=1aNMOeZZbNwSRkjyCWAGtNCMa3YrshR-n&export=download\",\n",
    "            \"sentiments\": \"https://drive.google.com/uc?id=1vkQS5gI0is4ACU58-AbWusnemw7KZNfO&export=download\",\n",
    "            \"topics\": \"https://drive.google.com/uc?id=1_ArMpDguVsbUGl-xSMkTF_p5KpZrmpSB&export=download\",\n",
    "        }\n",
    "    }\n",
    "\n",
    "    output_dir = \"dataset_feedback_students\"\n",
    "    csv_dir = os.path.join(output_dir, \"csv\")\n",
    "    os.makedirs(csv_dir, exist_ok=True)\n",
    "\n",
    "    datasets = {}\n",
    "\n",
    "    for split_name, split_urls in urls.items():\n",
    "        print(f\"\\n=== Processing {split_name} split ===\")\n",
    "        paths = {}\n",
    "        for data_type, url in split_urls.items():\n",
    "            local_path = os.path.join(output_dir, f\"{split_name}_{data_type}.txt\")\n",
    "            if not os.path.exists(local_path):\n",
    "                if download_from_drive(url, local_path):\n",
    "                    paths[data_type] = local_path\n",
    "                else:\n",
    "                    print(f\"Failed to download {data_type} for {split_name}\")\n",
    "            else:\n",
    "                print(f\"‚úì File already exists: {local_path}\")\n",
    "                paths[data_type] = local_path\n",
    "\n",
    "        if len(paths) != 3:\n",
    "            print(f\"Missing files for {split_name}, skipping...\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            with open(paths[\"sentences\"], 'r', encoding='utf-8') as f:\n",
    "                sentences = [line.strip() for line in f.readlines()]\n",
    "            with open(paths[\"sentiments\"], 'r', encoding='utf-8') as f:\n",
    "                sentiments = [line.strip() for line in f.readlines()]\n",
    "            with open(paths[\"topics\"], 'r', encoding='utf-8') as f:\n",
    "                topics = [line.strip() for line in f.readlines()]\n",
    "\n",
    "            if len(sentences) == len(sentiments) == len(topics):\n",
    "                # Map numeric -> text n·∫øu c·∫ßn\n",
    "                sentiment_map = {'0': 'negative', '1': 'neutral', '2': 'positive'}\n",
    "                sentiments = [sentiment_map.get(s.strip(), s.strip()) for s in sentiments]\n",
    "\n",
    "                df = pd.DataFrame({\n",
    "                    'sentence': sentences,\n",
    "                    'sentiment': sentiments,\n",
    "                    'topic': topics\n",
    "                })\n",
    "\n",
    "                csv_path = os.path.join(csv_dir, f\"{split_name}.csv\")\n",
    "                df.to_csv(csv_path, index=False, encoding='utf-8')\n",
    "                datasets[split_name] = df\n",
    "                print(f\"‚úì Saved {split_name}.csv with {len(df)} records\")\n",
    "            else:\n",
    "                print(f\"‚úó Data length mismatch in {split_name}: sentences={len(sentences)}, sentiments={len(sentiments)}, topics={len(topics)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Error processing {split_name}: {e}\")\n",
    "\n",
    "    return datasets\n",
    "\n",
    "# ---------------------------\n",
    "# 2) Text preprocessing\n",
    "# ---------------------------\n",
    "try:\n",
    "    from underthesea import word_tokenize\n",
    "    USE_UNDERTHESEA = True\n",
    "    print(\"‚úì underthesea available\")\n",
    "except Exception:\n",
    "    USE_UNDERTHESEA = False\n",
    "    print(\"‚ö† underthesea not available. Using simple tokenization.\")\n",
    "\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text).lower()\n",
    "    text = unicodedata.normalize('NFC', text)\n",
    "    text = re.sub(r'http\\S+|www\\S+', ' ', text)\n",
    "    text = re.sub(r'[@#]\\w+', ' ', text)\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text, flags=re.UNICODE)\n",
    "    text = re.sub(r'\\d+', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def tokenize_vietnamese(text):\n",
    "    text = clean_text(text)\n",
    "    if USE_UNDERTHESEA and text:\n",
    "        try:\n",
    "            return word_tokenize(text, format=\"text\")\n",
    "        except:\n",
    "            pass\n",
    "    return \" \".join(text.split())\n",
    "\n",
    "# ---------------------------\n",
    "# 3) EDA helper\n",
    "# ---------------------------\n",
    "def perform_eda(df, name=\"dataset\"):\n",
    "    print(f\"\\n=== EDA for {name} ===\")\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(\"\\nSentiment distribution:\")\n",
    "    print(df['sentiment'].value_counts().sort_index())\n",
    "    df['sentence_length'] = df['sentence'].astype(str).apply(lambda x: len(x.split()))\n",
    "    print(f\"\\nSentence length statistics:\")\n",
    "    print(df['sentence_length'].describe())\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18,5))\n",
    "    sentiment_counts = df['sentiment'].value_counts().sort_index()\n",
    "    axes[0].bar(sentiment_counts.index, sentiment_counts.values)\n",
    "    axes[0].set_title(f'{name} - Sentiment Distribution')\n",
    "    axes[0].set_xlabel('Sentiment')\n",
    "    axes[0].set_ylabel('Count')\n",
    "\n",
    "    top_topics = df['topic'].value_counts().head(10)\n",
    "    axes[1].barh(range(len(top_topics)), top_topics.values)\n",
    "    axes[1].set_yticks(range(len(top_topics)))\n",
    "    axes[1].set_yticklabels(top_topics.index)\n",
    "    axes[1].set_title(f'{name} - Top 10 Topics')\n",
    "    axes[1].set_xlabel('Count')\n",
    "\n",
    "    axes[2].hist(df['sentence_length'], bins=30, alpha=0.7)\n",
    "    axes[2].set_title(f'{name} - Sentence Length Distribution')\n",
    "    axes[2].set_xlabel('Sentence Length (words)')\n",
    "    axes[2].set_ylabel('Frequency')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return df\n",
    "\n",
    "def predict_sentiment(text, model, vectorizer):\n",
    "    text_clean = tokenize_vietnamese(text)\n",
    "    X = vectorizer.transform([text_clean])\n",
    "    pred = model.predict(X)[0]\n",
    "    prob = None\n",
    "    try:\n",
    "        prob = model.predict_proba(X)[0]\n",
    "    except:\n",
    "        pass\n",
    "    label_names = ['negative','neutral','positive']\n",
    "    return {\n",
    "        'prediction': label_names[pred],\n",
    "        'confidence': float(max(prob)) if prob is not None else None,\n",
    "        'probabilities': dict(zip(label_names, prob)) if prob is not None else None\n",
    "    }\n",
    "\n",
    "# ---------------------------\n",
    "# 6) Main\n",
    "# ---------------------------\n",
    "def main():\n",
    "    print(\"=== Vietnamese Student Feedback Sentiment Analysis (Oversampling) ===\")\n",
    "    datasets = prepare_dataset()\n",
    "    if len(datasets) < 3:\n",
    "        print(\"‚úó Not enough datasets. Aborting.\")\n",
    "        return\n",
    "\n",
    "    train_df = datasets['train']\n",
    "    val_df = datasets['validation']\n",
    "    test_df = datasets['test']\n",
    "    print(f\"‚úì Loaded datasets - Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n",
    "\n",
    "    train_df = perform_eda(train_df, \"Training\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "       main()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n‚ö† Interrupted by user\")\n",
    "    except Exception as e:\n",
    "        print(\"\\n‚úó Error during execution:\", e)\n",
    "        import traceback; traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eebb67c",
   "metadata": {},
   "source": [
    "# TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88f7668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "\n",
    "# ==============================\n",
    "# 1. Load dataset t·ª´ CSV\n",
    "# ==============================\n",
    "df_train = pd.read_csv(\"dataset_feedback_students/csv/train.csv\")\n",
    "df_validation = pd.read_csv(\"dataset_feedback_students/csv/validation.csv\")\n",
    "df_test = pd.read_csv(\"dataset_feedback_students/csv/test.csv\")\n",
    "\n",
    "print(\"K√≠ch th∆∞·ªõc t·∫≠p train:\", df_train.shape)\n",
    "print(\"K√≠ch th∆∞·ªõc t·∫≠p validation:\", df_validation.shape)\n",
    "print(\"K√≠ch th∆∞·ªõc t·∫≠p test:\", df_test.shape)\n",
    "\n",
    "# ==============================\n",
    "# 2. Chu·∫©n h√≥a label\n",
    "# ==============================\n",
    "# Encode sentiment -> s·ªë\n",
    "sentiment_encoder = LabelEncoder()\n",
    "df_train['sentiment'] = sentiment_encoder.fit_transform(df_train['sentiment'])\n",
    "df_validation['sentiment'] = sentiment_encoder.transform(df_validation['sentiment'])\n",
    "df_test['sentiment'] = sentiment_encoder.transform(df_test['sentiment'])\n",
    "\n",
    "print(\"C√°c nh√£n sentiment:\", list(sentiment_encoder.classes_))\n",
    "\n",
    "# ==============================\n",
    "# 3. Chu·∫©n b·ªã d·ªØ li·ªáu\n",
    "# ==============================\n",
    "X_train_text = df_train['sentence'].tolist()\n",
    "y_train = df_train[['sentiment', 'topic']].values\n",
    "\n",
    "X_validation_text = df_validation['sentence'].tolist()\n",
    "y_validation = df_validation[['sentiment', 'topic']].values\n",
    "\n",
    "X_test_text = df_test['sentence'].tolist()\n",
    "y_test = df_test[['sentiment', 'topic']].values\n",
    "\n",
    "# ==============================\n",
    "# 4. SBERT embedding\n",
    "# ==============================\n",
    "print(\"ƒêang t·∫£i model SBERT...\")\n",
    "sbert_model = SentenceTransformer(\"sentence-transformers/LaBSE\")\n",
    "\n",
    "print(\"ƒêang t·∫°o embedding cho d·ªØ li·ªáu train...\")\n",
    "X_train_embeddings = sbert_model.encode(X_train_text, convert_to_numpy=True, show_progress_bar=True)\n",
    "\n",
    "print(\"ƒêang t·∫°o embedding cho d·ªØ li·ªáu validation...\")\n",
    "X_validation_embeddings = sbert_model.encode(X_validation_text, convert_to_numpy=True, show_progress_bar=True)\n",
    "\n",
    "print(\"ƒêang t·∫°o embedding cho d·ªØ li·ªáu test...\")\n",
    "X_test_embeddings = sbert_model.encode(X_test_text, convert_to_numpy=True, show_progress_bar=True)\n",
    "\n",
    "print(\"K√≠ch th∆∞·ªõc embedding train:\", X_train_embeddings.shape)\n",
    "print(\"K√≠ch th∆∞·ªõc embedding validation:\", X_validation_embeddings.shape)\n",
    "print(\"K√≠ch th∆∞·ªõc embedding test:\", X_test_embeddings.shape)\n",
    "\n",
    "# ==============================\n",
    "# 5. Hu·∫•n luy·ªán m√¥ h√¨nh BalancedRandomForest\n",
    "# ==============================\n",
    "brf = BalancedRandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=15,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    sampling_strategy=\"auto\",\n",
    "    replacement=True\n",
    ")\n",
    "\n",
    "\n",
    "model = MultiOutputClassifier(brf)\n",
    "model.fit(X_train_embeddings, y_train)\n",
    "\n",
    "# ==============================\n",
    "# 6. ƒê√°nh gi√° m√¥ h√¨nh\n",
    "# ==============================\n",
    "print(\"ƒê√°nh gi√° tr√™n t·∫≠p validation...\")\n",
    "y_validation_pred = model.predict(X_validation_embeddings)\n",
    "\n",
    "print(\"\\nüìå K·∫øt qu·∫£ tr√™n t·∫≠p validation cho nh√£n 'sentiment':\")\n",
    "print(classification_report(y_validation[:, 0], y_validation_pred[:, 0], target_names=sentiment_encoder.classes_))\n",
    "\n",
    "print(\"\\nüìå K·∫øt qu·∫£ tr√™n t·∫≠p validation cho nh√£n 'topic':\")\n",
    "print(classification_report(y_validation[:, 1], y_validation_pred[:, 1]))\n",
    "\n",
    "validation_accuracy = (y_validation_pred == y_validation).all(axis=1).mean()\n",
    "print(f\"\\nüéØ ƒê·ªô ch√≠nh x√°c t·ªïng th·ªÉ tr√™n validation: {validation_accuracy:.4f}\")\n",
    "\n",
    "# ƒê√°nh gi√° tr√™n t·∫≠p test\n",
    "print(\"\\nƒê√°nh gi√° tr√™n t·∫≠p test...\")\n",
    "y_test_pred = model.predict(X_test_embeddings)\n",
    "\n",
    "print(\"\\nüìå K·∫øt qu·∫£ tr√™n t·∫≠p test cho nh√£n 'sentiment':\")\n",
    "print(classification_report(y_test[:, 0], y_test_pred[:, 0], target_names=sentiment_encoder.classes_))\n",
    "\n",
    "print(\"\\nüìå K·∫øt qu·∫£ tr√™n t·∫≠p test cho nh√£n 'topic':\")\n",
    "print(classification_report(y_test[:, 1], y_test_pred[:, 1]))\n",
    "\n",
    "test_accuracy = (y_test_pred == y_test).all(axis=1).mean()\n",
    "print(f\"\\nüéØ ƒê·ªô ch√≠nh x√°c t·ªïng th·ªÉ tr√™n test: {test_accuracy:.4f}\")\n",
    "\n",
    "# ==============================\n",
    "# 7. V·∫Ω ma tr·∫≠n nh·∫ßm l·∫´n\n",
    "# ==============================\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# Sentiment\n",
    "plt.subplot(1, 2, 1)\n",
    "cm_sentiment = confusion_matrix(y_test[:, 0], y_test_pred[:, 0])\n",
    "sns.heatmap(cm_sentiment, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=sentiment_encoder.classes_,\n",
    "            yticklabels=sentiment_encoder.classes_)\n",
    "plt.title('Ma tr·∫≠n nh·∫ßm l·∫´n - Sentiment')\n",
    "plt.ylabel('Th·ª±c t·∫ø')\n",
    "plt.xlabel('D·ª± ƒëo√°n')\n",
    "\n",
    "# Topic\n",
    "plt.subplot(1, 2, 2)\n",
    "cm_topic = confusion_matrix(y_test[:, 1], y_test_pred[:, 1])\n",
    "sns.heatmap(cm_topic, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Ma tr·∫≠n nh·∫ßm l·∫´n - Topic')\n",
    "plt.ylabel('Th·ª±c t·∫ø')\n",
    "plt.xlabel('D·ª± ƒëo√°n')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# # ==============================\n",
    "# # 8. L∆∞u m√¥ h√¨nh + encoder\n",
    "# # ==============================\n",
    "# joblib.dump({\"model\": model, \"sentiment_encoder\": sentiment_encoder}, \"multioutput_brf_model.pkl\")\n",
    "# print(\"‚úÖ ƒê√£ l∆∞u m√¥ h√¨nh v√†o multioutput_brf_model.pkl\")\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 8. L∆∞u m√¥ h√¨nh + encoder + t√™n SBERT\n",
    "# ==============================\n",
    "save_obj = {\n",
    "    \"model\": model,                         # m√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán\n",
    "    \"sentiment_encoder\": sentiment_encoder, # encoder cho sentiment\n",
    "    \"sbert_model_name\": \"sentence-transformers/LaBSE\"  # ch·ªâ c·∫ßn l∆∞u t√™n model\n",
    "}\n",
    "\n",
    "joblib.dump(save_obj, \"multioutput_brf_V2_model.pkl\")\n",
    "print(\"‚úÖ ƒê√£ l∆∞u m√¥ h√¨nh v√†o multioutput_brf_V2_model.pkl\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envBTL_ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
